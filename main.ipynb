{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import scipy.stats as stats\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_mean = 0\n",
    "price_std = 0\n",
    "\n",
    "def visualize_dataframe_distributions(df):\n",
    "    # Filtering columns by data type\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    datetime_cols = df.select_dtypes(include=['datetime64']).columns\n",
    "    \n",
    "    n = len(df.columns)  # Total number of columns\n",
    "    side_length = int(np.ceil(np.sqrt(n)))  # Calculating grid side length\n",
    "    \n",
    "    # Creating the subplot grid\n",
    "    fig, axes = plt.subplots(nrows=side_length, ncols=side_length, figsize=(50, 50))\n",
    "    fig.tight_layout(pad=4.0)\n",
    "    \n",
    "    for ax in axes.ravel():\n",
    "        ax.axis('off')  # Hide all axes initially\n",
    "\n",
    "    # Plotting numeric columns\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        ax = axes[i // side_length, i % side_length]\n",
    "        ax.axis('on')\n",
    "        ax.hist(df[col], bins=30, edgecolor='k')\n",
    "        ax.set_title(col)\n",
    "    \n",
    "    # Plotting categorical columns\n",
    "    for i, col in enumerate(categorical_cols, start=len(numeric_cols)):\n",
    "        ax = axes[i // side_length, i % side_length]\n",
    "        ax.axis('on')\n",
    "        df[col].value_counts().plot(kind='bar', ax=ax)\n",
    "        ax.set_title(col)\n",
    "    \n",
    "    # Plotting datetime columns\n",
    "    for i, col in enumerate(datetime_cols, start=len(numeric_cols) + len(categorical_cols)):\n",
    "        ax = axes[i // side_length, i % side_length]\n",
    "        ax.axis('on')\n",
    "        df[col].value_counts().sort_index().plot(ax=ax)\n",
    "        ax.set_title(col)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "def minmax_norm(df, variables_reales):\n",
    "    for variable in variables_reales:\n",
    "        df[variable] = (df[variable] - df[variable].min()) / (df[variable].max() - df[variable].min())\n",
    "    return df\n",
    "\n",
    "def zscore_norm(df, variables_reales):\n",
    "    for variable in variables_reales:\n",
    "        df[variable] = (df[variable] - df[variable].mean()) / df[variable].std()\n",
    "    return df\n",
    "\n",
    "def one_hot_encoding(df, variables_categoricas):\n",
    "    return pd.get_dummies(df, columns=variables_categoricas, dtype=np.int64)\n",
    "\n",
    "def extract_postal_hierarchy(df):\n",
    "    df['CP'] = df['CP'].astype(str)\n",
    "    df['postal_group'] = df['CP'].str[0]\n",
    "    df['region'] = df['CP'].str[0:2]\n",
    "    return df\n",
    "\n",
    "def zscore_norm_price(df):\n",
    "    global price_mean, price_std\n",
    "    price_mean = df['Precio'].mean()\n",
    "    price_std = df['Precio'].std()\n",
    "    df['Precio'] = (df['Precio'] - price_mean) / price_std\n",
    "    return df\n",
    "\n",
    "def zscore_norm_price_inverse(np_array):\n",
    "    global price_mean, price_std\n",
    "    return np_array * price_std + price_mean\n",
    "\n",
    "def plot_correlation_matrix(df):\n",
    "    corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    ax.matshow(corr, cmap='seismic', vmin=0, vmax=1)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation='vertical')\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.show()\n",
    "\n",
    "def x_y_split(df, target):\n",
    "    return df.drop(target, axis=1), df[target]\n",
    "\n",
    "def train_model(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def test_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def visualize_test(y_test, y_pred, ax, model_name):\n",
    "    perc_diff = abs((y_pred - y_test) / y_test) * 100\n",
    "    sorted_indexes = np.argsort(perc_diff)[::-1] \n",
    "    \n",
    "    y_test_sorted = y_test.iloc[sorted_indexes]\n",
    "    y_pred_sorted = y_pred[sorted_indexes]\n",
    "    \n",
    "    # paired = sorted(list(zip(y_test, y_pred)))\n",
    "    # y_test_sorted, y_pred_sorted = zip(*paired)\n",
    "\n",
    "    mae = mean_absolute_error(y_test_sorted, y_pred_sorted)\n",
    "    mape = mean_absolute_percentage_error(y_test_sorted, y_pred_sorted)\n",
    "\n",
    "    print('MAE: {:}'.format(mae/1000))\n",
    "    print('MAPE: {:.4}'.format(mape*100))\n",
    "\n",
    "    num_range = np.arange(0, len(y_test))\n",
    "    ax.plot(num_range, y_test_sorted, label='y_test', marker='*', color='blue')\n",
    "    ax.plot(num_range, y_pred_sorted, label='y_pred', marker='.', color='red')\n",
    "    ax.set_title(f'y_test vs y_pred {model_name}')\n",
    "    ax.legend()\n",
    "    \n",
    "    return sorted_indexes\n",
    "\n",
    "\n",
    "def plot_against_precio(df):\n",
    "    \"\"\"\n",
    "    Plot all columns in dataframe df against 'Precio'.\n",
    "    Assumes that 'Precio' is one of the columns in df.\n",
    "    \"\"\"\n",
    "    # Ensure that Seaborn's aesthetics are being used\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Number of columns to be plotted against 'Precio'\n",
    "    n = df.shape[1] - 1  # subtract 1 because we won't plot 'Precio' against itself\n",
    "    \n",
    "    # Create subplots; You can adjust the size (e.g., figsize) as per your requirements\n",
    "    fig, axes = plt.subplots(n, 1, figsize=(8, 5*n))\n",
    "    \n",
    "    # For every column other than 'Precio', plot it against 'Precio'\n",
    "    for i, column in enumerate([col for col in df.columns if col != 'Precio']):\n",
    "        sns.scatterplot(data=df, x='Precio', y=column, ax=axes[i])\n",
    "        axes[i].set_title(f'{column} vs. Precio')\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for nan value\n",
    "nans = df.isna().sum()\n",
    "nans = nans[nans > 0]\n",
    "print(nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Id', inplace=True)\n",
    "df.drop(['AguaCorriente', 'GasNatural', 'FosaSeptica', 'Piscina', 'Plan', 'PerimParcela'], axis=1, inplace=True)\n",
    "\n",
    "df = extract_postal_hierarchy(df)\n",
    "\n",
    "# df['Reformada'] = df['FechaConstruccion'] != df['FechaReforma']\n",
    "# df['Reformada'] = df['Reformada'].astype(int)\n",
    "\n",
    "# make garaje feature inverse, 0 is 1, 1 is 0\n",
    "df['SinGaraje'] = df['Garaje'].apply(lambda x: 1 if x == 0 else 0)\n",
    "df['3plantas'] = df['Plantas'].apply(lambda x: 1 if x == 3 else 0)\n",
    "\n",
    "# Current Year\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "df['AgeOfHouse'] = current_year - df['FechaConstruccion']\n",
    "df['YearsSinceReform'] = current_year - df['FechaReforma']\n",
    "#df['TotalRooms'] = df['Aseos'] + df['Habitaciones']\n",
    "#df['AvgProximity'] = (df['ProxCarretera'] + df['ProxCallePrincipal'] + df['ProxViasTren']) / 3\n",
    "\n",
    "df['aseos+hab*rating'] = (0.7*df['Aseos'] + 0.3*df['Habitaciones']) * df['RatingEstrellas']\n",
    "df['synth4'] = np.log1p(df['aseos+hab*rating'] * df['Superficie'])\n",
    "df['synth5'] = np.log1p(df['aseos+hab*rating'] * df['Estado'])\n",
    "df['synth6'] = np.log1p(df['Estado'] * df['Superficie'])\n",
    "df['synth7'] = np.log1p(df['aseos+hab*rating'] * df['Superficie'] * df['Estado'])\n",
    "\n",
    "#df['rooms/rating'] = df['Habitaciones'] / df['RatingEstrellas']\n",
    "\n",
    "df.drop(['FechaConstruccion', 'FechaReforma', 'Garaje', 'Formato', 'TipoDesnivel', 'Desnivel', 'Situacion', 'Plantas', 'PAU', 'Vallada', 'Callejon', 'CallePavimentada', 'Aseos', 'Habitaciones'], axis=1, inplace=True)\n",
    "\n",
    "variables_reales = df.columns[df.dtypes == 'float64']\n",
    "variables_categoricas = df.dtypes[df.dtypes == 'object'].index\n",
    "variables_enteras = df.columns[df.dtypes == 'int64']\n",
    "\n",
    "#visualize_dataframe_distributions(df)\n",
    "#plot_against_precio(df)\n",
    "#sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.pairplot(df)\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n",
    "\n",
    "#Use pairplot\n",
    "# df.drop(variables_categoricas, axis=1, inplace=True)\n",
    "df = one_hot_encoding(df, variables_categoricas)\n",
    "\n",
    "df.drop(['Tipo_Chalet individual', 'CatParcela_Residencial tipo 2', 'CatParcela_Residencial unifamiliar', 'CP_50012', 'CP_50018', 'CP_60645', 'CP_61704', 'CP_62451'], axis=1, inplace=True)\n",
    "\n",
    "#plot_against_precio(df)\n",
    "\n",
    "\n",
    "\n",
    "# from scipy.stats import skew\n",
    "# numeric_features = df.dtypes[df.dtypes != object].index\n",
    "# numeric_features = numeric_features.drop('Precio')\n",
    "# skewed_features = df[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "# high_skew = skewed_features[skewed_features > 0.5]\n",
    "# skew_index = high_skew.index\n",
    "\n",
    "# # Normalize skewed features using log_transformation\n",
    "    \n",
    "# for i in skew_index:\n",
    "#     df[i] = np.log1p(df[i])\n",
    "\n",
    "# inf=pd.DataFrame(np.isinf(df).sum() ,columns=['Inf_sum'])\n",
    "# inf['feat']=inf.index\n",
    "# inf=inf[inf['Inf_sum']>0]\n",
    "# inf=inf.sort_values(by=['Inf_sum'])\n",
    "# inf.insert(0,'Serial No.',range(1,len(inf)+1))\n",
    "# print(inf)\n",
    "\n",
    "variables_enteras = variables_enteras.drop('Precio')\n",
    "df = zscore_norm(df, variables_reales)\n",
    "df = zscore_norm(df, variables_enteras)\n",
    "\n",
    "#df = df[['RatingEstrellas', 'Superficie', 'Aseos', 'AgeOfHouse', 'YearsSinceReform', 'Habitaciones', 'Formato_Poco irregular', 'Situacion_Interior', 'Plantas', 'Formato_Rectangular', 'postal_group_5', 'Precio']]\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "df.to_csv('train_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = df.corr()\n",
    "# umbral = 0.05\n",
    "# # Encontrar características altamente correlacionadas\n",
    "# caract_alta_correlacion = set()\n",
    "# for i in range(len(corr.columns)):\n",
    "#     for j in range(i):\n",
    "#         if abs(corr.iloc[i, j]) < umbral:\n",
    "#             colname = corr.columns[i]\n",
    "#             caract_alta_correlacion.add(colname)\n",
    "# caract_alta_correlacion.remove('Precio')\n",
    "# # Eliminar características altamente correlacionadas\n",
    "# print(caract_alta_correlacion)\n",
    "# df = df.drop(caract_alta_correlacion, axis=1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train = df.sample(frac=train_size, random_state=1)\n",
    "test = df.drop(train.index)\n",
    "\n",
    "#train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformaciones del input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['Precio'] < 10**6]\n",
    "\n",
    "#train = train[train['Superficie'] < 3]\n",
    "\n",
    "#train['Superficie'] = np.log1p(train['Superficie'])\n",
    "\n",
    "#Calculate the number of bins\n",
    "# num_bins = 10\n",
    "# bins = np.linspace(train['Precio'].min(), train['Precio'].max(), num_bins+1)\n",
    "# mid_points = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# # Bin the data\n",
    "# train['binned'] = pd.cut(train['Precio'], bins=bins)\n",
    "# print(train['binned'].value_counts())\n",
    "# # Desired normal distribution parameters\n",
    "# mean = train['Precio'].mean()\n",
    "# std = train['Precio'].std()\n",
    "\n",
    "# # Calculate the desired number of samples per bin based on normal distribution\n",
    "# pdf_values = stats.norm.pdf(mid_points, mean, std)\n",
    "# desired_samples_per_bin = pdf_values / sum(pdf_values) * len(train)\n",
    "# print(desired_samples_per_bin)\n",
    "# # ... [rest of your code remains the same up to the 'desired_samples_per_bin' calculation]\n",
    "\n",
    "# # Get unique bin categories\n",
    "# unique_bins = train['binned'].cat.categories\n",
    "\n",
    "# # Oversample each bin\n",
    "# oversampled_dfs = []\n",
    "\n",
    "# for i, bin_category in enumerate(unique_bins):\n",
    "#     bin_df = train[train['binned'] == bin_category]\n",
    "#     num_samples = int(desired_samples_per_bin[i])\n",
    "\n",
    "#     if not bin_df.empty and num_samples > 0:\n",
    "#         oversampled_dfs.append(bin_df.sample(num_samples, replace=True))\n",
    "\n",
    "# if oversampled_dfs:\n",
    "#     oversampled_train = pd.concat(oversampled_dfs, axis=0).reset_index(drop=True)\n",
    "#     # Drop the 'binned' column\n",
    "#     oversampled_train = oversampled_train.drop(columns=['binned'])\n",
    "# else:\n",
    "#     print(\"No bins met the criteria for oversampling.\")\n",
    "\n",
    "\n",
    "#visualize_dataframe_distributions(oversampled_train)\n",
    "\n",
    "#train = oversampled_train\n",
    "\n",
    "n_iterations = 100  # number of bootstrap samples you want to create\n",
    "bootstrap_samples = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    bootstrap_sample = train.sample(n=len(train), replace=True)\n",
    "    bootstrap_samples.append(bootstrap_sample)\n",
    "\n",
    "train = pd.concat(bootstrap_samples, axis=0).reset_index(drop=True)\n",
    "\n",
    "#visualize_dataframe_distributions(train)\n",
    "\n",
    "train['Precio'] = np.log1p(train['Precio'])\n",
    "\n",
    "X_train, y_train = x_y_split(train, 'Precio')\n",
    "X_test, y_test = x_y_split(test, 'Precio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combination = {'recall': 0, 'model': None, 'norm': None, 'sampling': None, 'trained_model': None}\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))\n",
    "\n",
    "model = LinearRegression()\n",
    "model_name = model.__class__.__name__\n",
    "trained_model = train_model(model, X_train, y_train)\n",
    "y_pred = test_model(trained_model, X_test, y_test)\n",
    "\n",
    "y_pred = np.expm1(y_pred)\n",
    "\n",
    "indexes = visualize_test(y_test, y_pred, ax=axes, model_name='LinearRegression')\n",
    "\n",
    "#remove the first two indexes\n",
    "indexes = indexes[2:]\n",
    "\n",
    "y_test_2 = y_test.iloc[indexes]\n",
    "y_pred_2 = y_pred[indexes]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))\n",
    "indexes2 = visualize_test(y_test_2, y_pred_2, ax=axes, model_name='LinearRegression')\n",
    "\n",
    "#Get the top 10% of those indices\n",
    "top_10_percent_idx = indexes[:len(indexes) // 10]\n",
    "\n",
    "# Extract rows from X_test and y_test using these indices\n",
    "X_test_top_errors = X_test.iloc[top_10_percent_idx]\n",
    "y_test_top_errors = y_test.iloc[top_10_percent_idx]\n",
    "\n",
    "#Form a DataFrame\n",
    "\n",
    "df_top_errors = pd.concat([X_test_top_errors, y_test_top_errors], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_errors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'max_leaves': 8,\n",
    "          'depth': 3,\n",
    "          'od_wait': 200,\n",
    "          'l2_leaf_reg': 3,\n",
    "          'iterations': 200000,\n",
    "          'model_size_reg': 0.7,\n",
    "          'learning_rate': 0.05,\n",
    "          'random_seed': 42 }\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))\n",
    "final_model = CatBoostRegressor(**best_params)\n",
    "\n",
    "final_model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred = np.expm1(y_pred)\n",
    "\n",
    "visualize_test(y_test, y_pred, ax=axes, model_name='CatBoostRegressor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"modelo.pickle\", \"wb\") as handler:\n",
    "    pickle.dump(final_model, handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "catboost_regressor = CatBoostRegressor(\n",
    "    random_seed=0,  # Establece la semilla aleatoria\n",
    ")\n",
    "\n",
    "# Define tus hiperparámetros específicos de CatBoost\n",
    "param_grid = {\n",
    "    'depth': [6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'iterations': [500, 1000, 2000],\n",
    "    'l2_leaf_reg': [3, 5, 7],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bylevel': [0.7, 0.8, 0.9],\n",
    "    'border_count': [32, 64, 128]\n",
    "}\n",
    "\n",
    "# Realiza la búsqueda de hiperparámetros\n",
    "grid_search = GridSearchCV(catboost_regressor, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtiene el mejor modelo y los mejores hiperparámetros\n",
    "best_catboost_regressor = grid_search.best_estimator_\n",
    "best_param = grid_search.best_params_\n",
    "\n",
    "print(f\"Mejores Hiperparámetros: {best_param}\")\n",
    "\n",
    "y_pred = best_catboost_regressor.predict(X_test, y_test)\n",
    "\n",
    "y_pred = np.expm1(y_pred)\n",
    "indexes = visualize_test(y_test, y_pred, ax=axes, model_name='CatBoostRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LinearRegression(), RandomForestRegressor(), ElasticNet(), Lasso(), Ridge(), DecisionTreeRegressor(), KNeighborsRegressor(n_neighbors=3, weights='distance'), RandomForestRegressor(), GradientBoostingRegressor(learning_rate=0.01, n_estimators=1000), AdaBoostRegressor()]\n",
    "# cols and rows for subplots according to number of models\n",
    "fig, axes = plt.subplots(nrows=len(models), ncols=1, figsize=(14, len(models)*7))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model_name = model.__class__.__name__\n",
    "    trained_model = train_model(model, X_train, y_train)\n",
    "    y_pred = test_model(trained_model, X_test, y_test)\n",
    "    y_pred = np.expm1(y_pred)\n",
    "\n",
    "    visualize_test(y_test, y_pred, ax=axes[i], model_name=model_name)\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_boost = GradientBoostingRegressor()\n",
    "\n",
    "# distributions = {\n",
    "#     'learning_rate': uniform(0.01, 0.2),\n",
    "#     'n_estimators': randint(100, 1000),\n",
    "#     'max_depth': randint(3, 7),\n",
    "#     'min_samples_split': uniform(0.01, 0.2),\n",
    "#     'min_samples_leaf': uniform(0.01, 0.1),\n",
    "#     'subsample': uniform(0.8, 0.2),\n",
    "#     'max_features': ['auto', 'sqrt', 'log2', None]\n",
    "# }\n",
    "\n",
    "# clf = RandomizedSearchCV(grad_boost, distributions, random_state=0, n_iter=1000, verbose=1, n_jobs=-1, cv=5, scoring='neg_mean_squared_error')\n",
    "# search = clf.fit(X_train, y_train)\n",
    "# search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "# best_model = search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# y_pred = zscore_norm_price_inverse(y_pred)\n",
    "# visualize_test(zscore_norm_price_inverse(y_test), y_pred, ax=axes, model_name=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
